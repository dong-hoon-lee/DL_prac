{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'is', 'gold']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Time is gold'\n",
    "tokens = [x for x in s.split(' ')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hoon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'is', 'gold']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(s)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world.', 'Time to do some data science!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "s = 'Hello world.\\nTime to do some data science!'\n",
    "tokens = sent_tokenize(s)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram\n",
    "- n-gram: n개의 어절, 음절을 분리해 빈도수 분석\n",
    "- n=1 - unigram, n=2 - bigram, n=3 - trigram ... 보통 bigram을 가장 많이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'is'),\n",
       " ('is', 'no'),\n",
       " ('no', 'royal'),\n",
       " ('royal', 'way'),\n",
       " ('way', 'of'),\n",
       " ('of', 'learning')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "s = 'There is no royal way of learning'\n",
    "bigram = list(ngrams(sequence=s.split(), n=2))\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'is', 'no'),\n",
       " ('is', 'no', 'royal'),\n",
       " ('no', 'royal', 'way'),\n",
       " ('royal', 'way', 'of'),\n",
       " ('way', 'of', 'learning')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'There is no royal way of learning'\n",
    "trigram = list(ngrams(sequence=s.split(), n=3))\n",
    "trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS tagging\n",
    "- PoS: 품사를 의미함\n",
    "- 각 단어에 해당하는 품사를 태깅해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hoon/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Think like man of action and act like man of thought'\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Think', 'VBP'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('action', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('act', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('thought', 'NN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(s)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 제거\n",
    "- 불용어: a, the, on ... 등과같이 분석에 불필요한 단어들\n",
    "- 한글 불용어: 을, 를, 에게 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on', 'in', 'the']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = 'on in the'\n",
    "stop_words = stop_words.split(' ')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hoon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'do',\n",
       " 'not',\n",
       " 'walk',\n",
       " 'today',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'have',\n",
       " 'to',\n",
       " 'run',\n",
       " 'tomorrow']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'If you do not walk today, you will have to run tomorrow'\n",
    "words = word_tokenize(s)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If', 'walk', 'today', ',', 'run', 'tomorrow']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stopwords = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        no_stopwords.append(w)\n",
    "        \n",
    "no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming(어간 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'applic'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('application')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'begin'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('beginning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catch'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('catches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization(표제어 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/hoon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/hoon/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beginning'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('beginning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catch'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('catches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER(Named Entity Recognition, 개체명 인식)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/hoon/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /Users/hoon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rome', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('not', 'RB'),\n",
       " ('built', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('day', 'NN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Rome was not built in a day'\n",
    "tags = nltk.pos_tag(word_tokenize(s))\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NE Rome/NNP) was/VBD not/RB built/VBN in/IN a/DT day/NN)\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.ne_chunk(tags, binary=True)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Ambiguity(단어의 중의성)\n",
    "- 단어의 중의성 문제 해결을 위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw bats\n",
      "Synset('saw.v.01')\n",
      "Synset('squash_racket.n.01')\n"
     ]
    }
   ],
   "source": [
    "s = 'I saw bats'\n",
    "\n",
    "print(s)\n",
    "print(lesk(word_tokenize(s), 'saw'))\n",
    "print(lesk(word_tokenize(s), 'bats'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 Tokenization\n",
    "- 한국어는 띄어쓰기로 명확하게 구분되지 않음\n",
    "- 한국어에는 형태소가 존재함\n",
    "- 따라서 형태소를 추가로 고려해 토큰화 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeCab - 한국어 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tagger = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('언제나', 'MAG'),\n",
       " ('현재', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('집중', 'NNG'),\n",
       " ('할', 'XSV+ETM'),\n",
       " ('수', 'NNB'),\n",
       " ('있', 'VV'),\n",
       " ('다면', 'EC'),\n",
       " ('행복', 'NNG'),\n",
       " ('할', 'XSV+ETM'),\n",
       " ('것', 'NNB'),\n",
       " ('이', 'VCP'),\n",
       " ('다', 'EC')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '언제나 현재에 집중할 수 있다면 행복할것이다'\n",
    "tagger.pos(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['언제나', '현재', '에', '집중', '할', '수', '있', '다면', '행복', '할', '것', '이', '다']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.morphs(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tagger.nouns()\n",
    "- 형태소만 사용하기 위해 조사, 접속사가 제거된 문장 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['현재', '집중', '수', '행복', '것']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.nouns(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['내일 뭐하지?', '진짜? 이렇게 애매모호한 문장도 과연. 밥은 먹었어?', '나는...']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kss: Korean Sentence Splitter, 한국어 문장 분리기\n",
    "import kss\n",
    "\n",
    "txt = '내일 뭐하지? 진짜? 이렇게 애매모호한 문장도 과연. 밥은 먹었어? 나는...'\n",
    "kss.split_sentences(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 2 2 1 1]]\n",
      "{'think': 5, 'like': 2, 'man': 3, 'of': 4, 'action': 1, 'act': 0, 'thought': 6}\n"
     ]
    }
   ],
   "source": [
    "corpus = ['Think like a man of action act like man of thought']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# bow.toarray(): 인덱스에 해당하는 단어가 몇 번 등장했는지 알려줌\n",
    "print(bow.toarray())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Term Matrix(DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 2 2 0 1 1 0 0]\n",
      " [0 0 0 0 0 2 1 0 0 2 1]\n",
      " [0 0 1 1 0 0 0 0 0 0 0]]\n",
      "{'think': 7, 'like': 4, 'man': 5, 'action': 1, 'act': 0, 'thought': 8, 'try': 9, 'success': 6, 'value': 10, 'liberty': 3, 'death': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['Think like a man of action and act like man of thought',\n",
    "          'Try not to become a man of success but rather try to become a man of value',\n",
    "          'Give me liberty, or give me death']\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>death</th>\n",
       "      <th>liberty</th>\n",
       "      <th>like</th>\n",
       "      <th>man</th>\n",
       "      <th>success</th>\n",
       "      <th>think</th>\n",
       "      <th>thought</th>\n",
       "      <th>try</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   act  action  death  liberty  like  man  success  think  thought  try  value\n",
       "0    1       1      0        0     2    2        0      1        1    0      0\n",
       "1    0       0      0        0     0    2        1      0        0    2      1\n",
       "2    0       0      1        1     0    0        0      0        0    0      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = []\n",
    "\n",
    "for k, v in sorted(vectorizer.vocabulary_.items(), key=lambda item:item[1]):\n",
    "    cols.append(k)\n",
    "    \n",
    "df = pd.DataFrame(bow.toarray(), columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "- 특정 단어가 특정 문서에서 많이 등장하지만 다른 문서에서 등장 빈도가 적다면 해당 단어는 해당 문서에서의 핵심 단어로 간주하는 방법\n",
    "- tf-idf = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer(stop_words='english').fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf의 결과값이 크다면 해당 단어가 문서에서의 중요도가 높다고 해석할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.311383   0.311383   0.         0.         0.62276601 0.4736296\n",
      "  0.         0.311383   0.311383   0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.52753275\n",
      "  0.34682109 0.         0.         0.69364217 0.34682109]\n",
      " [0.         0.         0.70710678 0.70710678 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "{'think': 7, 'like': 4, 'man': 5, 'action': 1, 'act': 0, 'thought': 8, 'try': 9, 'success': 6, 'value': 10, 'liberty': 3, 'death': 2}\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf.transform(corpus).toarray())\n",
    "print(tf_idf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>death</th>\n",
       "      <th>liberty</th>\n",
       "      <th>like</th>\n",
       "      <th>man</th>\n",
       "      <th>success</th>\n",
       "      <th>think</th>\n",
       "      <th>thought</th>\n",
       "      <th>try</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.473630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527533</td>\n",
       "      <td>0.346821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693642</td>\n",
       "      <td>0.346821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        act    action     death   liberty      like       man   success  \\\n",
       "0  0.311383  0.311383  0.000000  0.000000  0.622766  0.473630  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.527533  0.346821   \n",
       "2  0.000000  0.000000  0.707107  0.707107  0.000000  0.000000  0.000000   \n",
       "\n",
       "      think   thought       try     value  \n",
       "0  0.311383  0.311383  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.693642  0.346821  \n",
       "2  0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = []\n",
    "\n",
    "for k, v in sorted(tf_idf.vocabulary_.items(), key=lambda item:item[1]):\n",
    "    cols.append(k)\n",
    "    \n",
    "pd.DataFrame(tf_idf.transform(corpus).toarray(), columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword analysis\n",
    "- keyword: 텍스트 자료의 중요한 내용을 압축적으로 제시하는 단어/문구\n",
    "- 불용어 제거, 어간추출, 형태소분석 등의 텍스트 전처리 과정을 거친 후에 분석 진행해야 함\n",
    "- 특정 텍스트 자료에 많이 나타나는 형태소가 해당 텍스트 문서 주제를 나타낼 가능성이 높다는 가정에 기초\n",
    "- 키워드 분석 사용 예시\n",
    "    - 텍스트의 주제 추정\n",
    "    - 텍스트 유사도\n",
    "    - 검색 엔진의 검색 우선순위 측정 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "from matplotlib import rc\n",
    "%matplotlib inline\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 영화 리뷰 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'id\\tdocument\\tlabel\\n',\n",
       " b'8112052\\t\\xec\\x96\\xb4\\xeb\\xa6\\xb4\\xeb\\x95\\x8c\\xeb\\xb3\\xb4\\xea\\xb3\\xa0 \\xec\\xa7\\x80\\xea\\xb8\\x88\\xeb\\x8b\\xa4\\xec\\x8b\\x9c\\xeb\\xb4\\x90\\xeb\\x8f\\x84 \\xec\\x9e\\xac\\xeb\\xb0\\x8c\\xec\\x96\\xb4\\xec\\x9a\\x94\\xe3\\x85\\x8b\\xe3\\x85\\x8b\\t1\\n',\n",
       " b'8132799\\t\\xeb\\x94\\x94\\xec\\x9e\\x90\\xec\\x9d\\xb8\\xec\\x9d\\x84 \\xeb\\xb0\\xb0\\xec\\x9a\\xb0\\xeb\\x8a\\x94 \\xed\\x95\\x99\\xec\\x83\\x9d\\xec\\x9c\\xbc\\xeb\\xa1\\x9c, \\xec\\x99\\xb8\\xea\\xb5\\xad\\xeb\\x94\\x94\\xec\\x9e\\x90\\xec\\x9d\\xb4\\xeb\\x84\\x88\\xec\\x99\\x80 \\xea\\xb7\\xb8\\xeb\\x93\\xa4\\xec\\x9d\\xb4 \\xec\\x9d\\xbc\\xea\\xb5\\xb0 \\xec\\xa0\\x84\\xed\\x86\\xb5\\xec\\x9d\\x84 \\xed\\x86\\xb5\\xed\\x95\\xb4 \\xeb\\xb0\\x9c\\xec\\xa0\\x84\\xed\\x95\\xb4\\xea\\xb0\\x80\\xeb\\x8a\\x94 \\xeb\\xac\\xb8\\xed\\x99\\x94\\xec\\x82\\xb0\\xec\\x97\\x85\\xec\\x9d\\xb4 \\xeb\\xb6\\x80\\xeb\\x9f\\xac\\xec\\x9b\\xa0\\xeb\\x8a\\x94\\xeb\\x8d\\xb0. \\xec\\x82\\xac\\xec\\x8b\\xa4 \\xec\\x9a\\xb0\\xeb\\xa6\\xac\\xeb\\x82\\x98\\xeb\\x9d\\xbc\\xec\\x97\\x90\\xec\\x84\\x9c\\xeb\\x8f\\x84 \\xea\\xb7\\xb8 \\xec\\x96\\xb4\\xeb\\xa0\\xa4\\xec\\x9a\\xb4\\xec\\x8b\\x9c\\xec\\xa0\\x88\\xec\\x97\\x90 \\xeb\\x81\\x9d\\xea\\xb9\\x8c\\xec\\xa7\\x80 \\xec\\x97\\xb4\\xec\\xa0\\x95\\xec\\x9d\\x84 \\xec\\xa7\\x80\\xed\\x82\\xa8 \\xeb\\x85\\xb8\\xeb\\x9d\\xbc\\xeb\\x85\\xb8 \\xea\\xb0\\x99\\xec\\x9d\\x80 \\xec\\xa0\\x84\\xed\\x86\\xb5\\xec\\x9d\\xb4\\xec\\x9e\\x88\\xec\\x96\\xb4 \\xec\\xa0\\x80\\xec\\x99\\x80 \\xea\\xb0\\x99\\xec\\x9d\\x80 \\xec\\x82\\xac\\xeb\\x9e\\x8c\\xeb\\x93\\xa4\\xec\\x9d\\xb4 \\xea\\xbf\\x88\\xec\\x9d\\x84 \\xea\\xbe\\xb8\\xea\\xb3\\xa0 \\xec\\x9d\\xb4\\xeb\\xa4\\x84\\xeb\\x82\\x98\\xea\\xb0\\x88 \\xec\\x88\\x98 \\xec\\x9e\\x88\\xeb\\x8b\\xa4\\xeb\\x8a\\x94 \\xea\\xb2\\x83\\xec\\x97\\x90 \\xea\\xb0\\x90\\xec\\x82\\xac\\xed\\x95\\xa9\\xeb\\x8b\\x88\\xeb\\x8b\\xa4.\\t1\\n',\n",
       " b'4655635\\t\\xed\\x8f\\xb4\\xeb\\xa6\\xac\\xec\\x8a\\xa4\\xec\\x8a\\xa4\\xed\\x86\\xa0\\xeb\\xa6\\xac \\xec\\x8b\\x9c\\xeb\\xa6\\xac\\xec\\xa6\\x88\\xeb\\x8a\\x94 1\\xeb\\xb6\\x80\\xed\\x84\\xb0 \\xeb\\x89\\xb4\\xea\\xb9\\x8c\\xec\\xa7\\x80 \\xeb\\xb2\\x84\\xeb\\xa6\\xb4\\xea\\xbb\\x98 \\xed\\x95\\x98\\xeb\\x82\\x98\\xeb\\x8f\\x84 \\xec\\x97\\x86\\xec\\x9d\\x8c.. \\xec\\xb5\\x9c\\xea\\xb3\\xa0.\\t1\\n',\n",
       " b'9251303\\t\\xec\\x99\\x80.. \\xec\\x97\\xb0\\xea\\xb8\\xb0\\xea\\xb0\\x80 \\xec\\xa7\\x84\\xec\\xa7\\x9c \\xea\\xb0\\x9c\\xec\\xa9\\x94\\xea\\xb5\\xac\\xeb\\x82\\x98.. \\xec\\xa7\\x80\\xeb\\xa3\\xa8\\xed\\x95\\xa0\\xea\\xb1\\xb0\\xeb\\x9d\\xbc\\xea\\xb3\\xa0 \\xec\\x83\\x9d\\xea\\xb0\\x81\\xed\\x96\\x88\\xeb\\x8a\\x94\\xeb\\x8d\\xb0 \\xeb\\xaa\\xb0\\xec\\x9e\\x85\\xed\\x95\\xb4\\xec\\x84\\x9c \\xeb\\xb4\\xa4\\xeb\\x8b\\xa4.. \\xea\\xb7\\xb8\\xeb\\x9e\\x98 \\xec\\x9d\\xb4\\xeb\\x9f\\xb0\\xea\\xb2\\x8c \\xec\\xa7\\x84\\xec\\xa7\\x9c \\xec\\x98\\x81\\xed\\x99\\x94\\xec\\xa7\\x80\\t1\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "raw = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt').readlines()\n",
    "raw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data:\n",
      "['8112052\\t어릴때보고 지금다시봐도 재밌어요ㅋㅋ\\t1\\n', '8132799\\t디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.\\t1\\n', '4655635\\t폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.\\t1\\n', '9251303\\t와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런게 진짜 영화지\\t1\\n']\n",
      "\n",
      "review data:\n",
      "['어릴때보고 지금다시봐도 재밌어요ㅋㅋ', '디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.', '폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.', '와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런게 진짜 영화지']\n"
     ]
    }
   ],
   "source": [
    "# raw[0]: column name이기 때문에 decode할 필요 없음\n",
    "raw = [x.decode() for x in raw[1:]]\n",
    "print(f'raw data:\\n{raw[:4]}\\n')\n",
    "\n",
    "reviews = []\n",
    "for i in raw:\n",
    "    reviews.append(i.split('\\t')[1])\n",
    "    \n",
    "print(f'review data:\\n{reviews[:4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
